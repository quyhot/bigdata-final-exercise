{
  "paragraphs": [
    {
      "title": "Loading data from CSV",
      "text": "%pyspark\nflightData2015 \u003d spark\\\n  .read\\\n  .option(\"inferSchema\", \"true\")\\\n  .option(\"header\", \"true\")\\\n  .csv(\"/usr/zeppelin/notebook/dataset/2015-summary.csv\")",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 7:05:01 AM",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1559819948881_-1163692008",
      "id": "20190519-074236_88673657",
      "dateCreated": "Jun 6, 2019 11:19:08 AM",
      "dateStarted": "Nov 25, 2019 7:05:01 AM",
      "dateFinished": "Nov 25, 2019 7:05:04 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Inspecting",
      "text": "%md\nIf we perform the take action on the DataFrame, we will be able to see the same results that we saw before when we used the command line",
      "dateUpdated": "Jun 6, 2019 11:19:08 AM",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eIf we perform the take action on the DataFrame, we will be able to see the same results that we saw before when we used the command line:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559819948965_-1109827162",
      "id": "20190519-080603_486601978",
      "dateCreated": "Jun 6, 2019 11:19:08 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "",
      "text": "%pyspark\nflightData2015.take(1)",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 7:05:16 AM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[Row(DEST_COUNTRY_NAME\u003d\u0027United States\u0027, ORIGIN_COUNTRY_NAME\u003d\u0027Romania\u0027, count\u003d15)]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559819948971_-1110596659",
      "id": "20190519-074258_1595130051",
      "dateCreated": "Jun 6, 2019 11:19:08 AM",
      "dateStarted": "Nov 25, 2019 7:05:16 AM",
      "dateFinished": "Nov 25, 2019 7:05:16 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nNothing happens to the data when we call sort because it’s just a transformation. However, we can see that Spark is building up a plan for how it will execute this across the cluster by looking at the explain plan. We can call explain on any DataFrame object to see the DataFrame’s lineage (or how Spark will execute this query):",
      "dateUpdated": "Jun 6, 2019 11:19:08 AM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eNothing happens to the data when we call sort because it’s just a transformation. However, we can see that Spark is building up a plan for how it will execute this across the cluster by looking at the explain plan. We can call explain on any DataFrame object to see the DataFrame’s lineage (or how Spark will execute this query):\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559819948975_-1112135655",
      "id": "20190519-075905_279128964",
      "dateCreated": "Jun 6, 2019 11:19:08 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.functions import max\nflightData2015.printSchema()\nflightData2015.select(max(\"count\")).take(1)",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 7:09:08 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- DEST_COUNTRY_NAME: string (nullable \u003d true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable \u003d true)\n |-- count: integer (nullable \u003d true)\n\n[Row(max(count)\u003d370002)]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559819948981_-1103671179",
      "id": "20190519-080715_1891193452",
      "dateCreated": "Jun 6, 2019 11:19:08 AM",
      "dateStarted": "Nov 25, 2019 7:09:08 AM",
      "dateFinished": "Nov 25, 2019 7:09:08 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Spark SQL",
      "text": "%pyspark\nflightData2015.createOrReplaceTempView(\"flight_data_2015\")",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 7:09:09 AM",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1559819948988_-1106364422",
      "id": "20190519-080740_622461850",
      "dateCreated": "Jun 6, 2019 11:19:08 AM",
      "dateStarted": "Nov 25, 2019 7:09:09 AM",
      "dateFinished": "Nov 25, 2019 7:09:09 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nmaxSql \u003d spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, sum(count) as destination_total\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\nORDER BY sum(count) DESC\nLIMIT 5\n\"\"\")\n\nmaxSql.show()",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 7:09:12 AM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------------+-----------------+\n|DEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n|    United States|           411352|\n|           Canada|             8399|\n|           Mexico|             7140|\n|   United Kingdom|             2025|\n|            Japan|             1548|\n+-----------------+-----------------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559819948990_-1105594924",
      "id": "20190519-080856_126343929",
      "dateCreated": "Jun 6, 2019 11:19:08 AM",
      "dateStarted": "Nov 25, 2019 7:09:12 AM",
      "dateFinished": "Nov 25, 2019 7:09:15 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\nSELECT DEST_COUNTRY_NAME, sum(count) as destination_total\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\nORDER BY sum(count) DESC\nLIMIT 5",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 7:10:11 AM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/sql",
        "results": {
          "0": {
            "graph": {
              "mode": "pieChart",
              "height": 300.0,
              "optionOpen": false
            },
            "helium": {}
          }
        },
        "enabled": true,
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "DEST_COUNTRY_NAME\tdestination_total\nUnited States\t411352\nCanada\t8399\nMexico\t7140\nUnited Kingdom\t2025\nJapan\t1548\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559819948994_-1119445884",
      "id": "20190519-081029_790674463",
      "dateCreated": "Jun 6, 2019 11:19:08 AM",
      "dateStarted": "Nov 25, 2019 7:09:17 AM",
      "dateFinished": "Nov 25, 2019 7:09:19 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%python\nimport sys\nfrom pyspark import SparkContext\nsc \u003d SparkContext(appName\u003d\"WordCountExample\")\nlines \u003d sc.textFile(sys.argv[1])\ncounts \u003d lines.flatMap(lambda x: x.split(\u0027 \u0027)) \\\n                  .map(lambda x: (x, 1)) \\\n                  .reduceByKey(lambda x,y:x+y)\noutput \u003d counts.collect()\nfor (word, count) in output:\n    print (\"%s: %i\" % (word, count))\nsc.stop()\n",
      "user": "anonymous",
      "dateUpdated": "Dec 7, 2021 1:48:26 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "paragraph_1559819948996_-1121754378\u0027s Interpreter pyspark not found"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559819948996_-1121754378",
      "id": "20190519-081829_446259091",
      "dateCreated": "Jun 6, 2019 11:19:08 AM",
      "status": "ERROR",
      "errorMessage": "org.apache.zeppelin.interpreter.InterpreterException: paragraph_1559819948996_-1121754378\u0027s Interpreter pyspark not found\n\tat org.apache.zeppelin.notebook.Note.run(Note.java:621)\n\tat org.apache.zeppelin.socket.NotebookServer.persistAndExecuteSingleParagraph(NotebookServer.java:1647)\n\tat org.apache.zeppelin.socket.NotebookServer.runParagraph(NotebookServer.java:1621)\n\tat org.apache.zeppelin.socket.NotebookServer.onMessage(NotebookServer.java:266)\n\tat org.apache.zeppelin.socket.NotebookSocket.onWebSocketText(NotebookSocket.java:59)\n\tat org.eclipse.jetty.websocket.common.events.JettyListenerEventDriver.onTextMessage(JettyListenerEventDriver.java:128)\n\tat org.eclipse.jetty.websocket.common.message.SimpleTextMessage.messageComplete(SimpleTextMessage.java:69)\n\tat org.eclipse.jetty.websocket.common.events.AbstractEventDriver.appendMessage(AbstractEventDriver.java:65)\n\tat org.eclipse.jetty.websocket.common.events.JettyListenerEventDriver.onTextFrame(JettyListenerEventDriver.java:122)\n\tat org.eclipse.jetty.websocket.common.events.AbstractEventDriver.incomingFrame(AbstractEventDriver.java:161)\n\tat org.eclipse.jetty.websocket.common.WebSocketSession.incomingFrame(WebSocketSession.java:309)\n\tat org.eclipse.jetty.websocket.common.extensions.ExtensionStack.incomingFrame(ExtensionStack.java:214)\n\tat org.eclipse.jetty.websocket.common.Parser.notifyFrame(Parser.java:220)\n\tat org.eclipse.jetty.websocket.common.Parser.parse(Parser.java:258)\n\tat org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.readParse(AbstractWebSocketConnection.java:632)\n\tat org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.onFillable(AbstractWebSocketConnection.java:480)\n\tat org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1638884850029_-608690758",
      "id": "20211207-134730_808786025",
      "dateCreated": "Dec 7, 2021 1:47:30 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/Big-data-class/Module-09/Flight-data",
  "id": "2EDEH8JHG",
  "angularObjects": {},
  "config": {},
  "info": {}
}